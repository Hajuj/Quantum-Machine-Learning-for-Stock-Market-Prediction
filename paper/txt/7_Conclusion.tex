In this study, our primary focus has been on the implementation and optimization of a QLSTM model. The foundational model is constructed to address the challenges associated with sequential forecasting. We systematically explore various architectures, examining the impact of key parameters, including the number of qubits utilized for input, the number of inputs for predicting future time steps, and variations in the entanglement layer, among other hyper-parameters. Through our investigation, we offer insights into how these choices influence the overall performance and accuracy of the QLSTM model.

A retrospective analysis of our Evaluation section reveals subtle distinctions among the different architectures, with certain modifications demonstrating marginal but discernible benefits to prediction outcomes. Despite the modest nature of these observed changes, they underscore the sensitivity of the model to specific parameter configurations.
	
This paper serves as a foundational exploration of the optimization of distinct QML models built for the challenges of sequential learning. While we have examined a multitude of hyper-parameters, there remains ample opportunity for further exploration. This research can be viewed as an initial step in the refinement of QML models for efficient time series prediction. Moreover, the methodology presented here can be extended to other models designed to address the problem of time series prediction, opening avenues for continued investigation into additional parameters and their impact on model performance.